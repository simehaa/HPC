# Chapter 11, Hybrid parallelization with MPI and OpenMP

Large-scale parallel computers are nowadays exclusively of the distributed-memory
type at the overall system level but use shared-memory compute nodes as basic building blocks.

## 11.1 Basic MPI/OpenMP programming models
Allow any MPI process to spawn a team of OpenMP threads. First, it must be specified the max number of threads each process can have.
Two basic hybrid approaches:
1. vector mode,
2. task mode.

### Vector mode implementation
Most general and allows any kind of MPI communication within OpenMP regions. One advantage: ease of programming.


### Task mode implementation
MPI communication within OpenMP parallel regions.

### Case study: Hybrid Jacobi solver
Recall the Jacobi solver. This simple case study reveals the most important rule of hybrid programming:
Consider going hybrid only if pure MPI scalability is not satisfactory. It does not
make sense to work hard on a hybrid implementation and try to be faster than a
perfectly scaling MPI code.

## 11.2 MPI taxonomy of thread interoperability

## 11.3 Hybrid decomposition and mapping
* One MPI process per node:
* One MPI process per socket:
* Multiple MPI processes per socket:
## 11.4 Potential benefits and drawbacks of hybrid programming
* Improved rate of convergence:
* Re-use of data in shared caches:
* Exploiting additional levels of parallelism:
* Overlapping MPI communication and computation:
* Reducing MPI overhead:
* Multiple levels of overhead:
* Bulk-synchronous communication in vector mode:
