# Chapter 10, Efficient MPI programming
Performance problems in a parallelized program are related to well-known general issues such as
* Serial execution (Amdahl's Law)
* Load imbalance
* Unnecessary synchronization
* Other effects that impact all parallel programming models

Specific MPI problems:
* Unjustified assumptions about distributed-memory parallelization
* Over-optimistic notions regarding the cost and side effects of communication
* Platform portability

This chapter: sketch the most relevant guidelines for efficient MPI programming.

## 10.1 MPI performance tools
The root cause is hard to find in an MPI program.
**IPM** is a simple and low-overhead tool that retrieves information such as time spent in the MPI library and application code, dominating functions etc.

More advanced profiling tools (such as Intel Trace Analyzer) can show for example *Event timeline* in a GUI.

## 10.2 Communication parameters
MPI message transfers are complex.
* Short messages: the message itself w/supplementary information may be sent and stored at the receivers side. At some point the intermediate buffer will be copied to the receiver buffer: *eager protocol*. Too many of these protocols may cause contention or program crashes.
* Large messages: envelope is immediately stored at the receiver, but the actual message transfer blocks until the user's receive buffer is available. Extra data copies could be avoided, improving effective bandwidth, but sender and receiver must synchronize. This is called the *rendezvous protocol*.

It could be useful to adjust message length:
`MPI_Issend()` is useful if *eager overflow* could be a problem.

## 10.3 Synchronization, serialization, contention
Performance problems that are not specific to message-passing.
### Implicit serialization and synchronization
Unintended frequent synchronization. Ring shift communication (rank 0->1->2->...->0): if this is periodic, there is a danger of deadlock. If blocking send/recv are used, the communication will take long time. SOLUTION:
* Have all odd/even send and all even/odd recv, then opposite.
* Use nonblocking functions.
* Use blocking point-to-point functions: notably MPI_Sendrecv() or MPI_Sendrecv_replace().
### Contention
Contention to network connections: shared resources within a shared-memory multisocket multicore system. Network contentions occurs on two levels:
* Multiple threads/processes may issue requests to other nodes. If bandwidth dies not scale to multiple connections: the bandwidth per connection will go down. (A single thread can saturate the network interface).
* The network topology may not be fully nonblocking.

Contention of some kind is hardly avoidable in parallel systems if message passing is used.

## 10.4 Reducing communication overhead
### Optimal domain decomposition
Halo communication should be efficient: minimize communicated data volume.
#### Minimizing interdomain surface area
Consider a cube domain of size $L^3$ and let there be $N$ sub-domains. The data volume is of $w$ bytes.
* Slabs (decomposed in one dimension): $2wL^2$
* Poles (decomposed in two dimensions): $4wL^2 N^{-1/2}$. ($N>4$ to beat slabs)
* Cubes (decomposed in three dimensions): $6wL^2N^{-2/3}.
#### Mapping issues
Modern parallel computers are of the hierarchical type. 'MPI_Cart_create()' can optimize mapping.
### Aggregating messages
Small messages should be aggregated into contiguous buffers and sent in larger chunk so that the latency penalty must only be paid once.
#### Message aggregation and derived datatypes

### Nonblocking vs. asynchronous communication
### Collective communication


## 10.5 Understanding intranode point-to-point communication
